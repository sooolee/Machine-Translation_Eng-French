{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation - English to French\n",
    "\n",
    "##### This project is to build an end-to-end machine translation pipeline that takes English text and retuns French translation. The model is built from scratch based on Encoder-Decoder and RNN structures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, LSTM, InputLayer, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The most common datasets used for machine translation are from WMT (http://www.statmt.org/).  However, that will take a long time to train a neural network on. To train the model in a reasonalbe time, a dataset that contains a small vocabulary has been chosen for this project.\n",
    "\n",
    "### Load Data\n",
    "The file path for English setneces is `data/small_vocab_en` and for French `data/small_vocab_fr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n",
    "\n",
    "# Load English data\n",
    "english_sentences = load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Samples\n",
    "Let's view the first two lines from each file (English and French)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sample Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "French Sample Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "English Sample Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "French Sample Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('English Sample Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('French Sample Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has been partially preprocessed. For example the puncuations have been delimited using spaces and the text have been converted to lowercase.  But the text requires more preprocessing.\n",
    "\n",
    "### Stats of the Dataset\n",
    "The dataset has relatively small vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data\n",
    "Following preprocessing steps need to be done:\n",
    "1. Tokenize the words into ids.\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate the language  specific tokenizer and tokenize the sentence.\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokenized_x = tokenizer.texts_to_sequences(x)\n",
    "    return tokenized_x, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to pad sentences to the same max length.\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if length == None:\n",
    "        length = max([len(sequence) for sequence in x])\n",
    "    padded_x = pad_sequences(x, maxlen = length, padding='post')\n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and Split Data into Train and Test Datasets\n",
    "Create a preprocess pipeline that tokenize and pad the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Input List of sentences\n",
    "    :param y: Target (Label) List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all English and French sentences. Calculate the maximun length for each language.\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into Train and Test datasets. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_english, test_english, train_french, test_french = train_test_split(\n",
    "    preproc_english_sentences, preproc_french_sentences, shuffle=False, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110288, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110288, 21, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_french.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture and Training\n",
    "\n",
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is based on Encoder-Decoder RNN incorporating embedding and bidirectional layers. For RNN units, both GRU and LSTM have been experimented and I ended up with LSTM, which took a bit longer to train but generated better outcomes. \n",
    "\n",
    "#### Hyperparameters\n",
    "Various values for hyperparameters were experimented. Embedding size, learning rate, epochs, GRU vs LSTM units, etc. Also 'relu' activation function was used for LSTM instead of its default 'tanh'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_dim = 300\n",
    "    learning_rate = 2e-3\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(english_vocab_size, embedding_dim, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(LSTM(256, activation='relu', return_sequences=False))) \n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(256, activation='relu', return_sequences=True))) \n",
    "    model.add(TimeDistributed(Dense(french_vocab_size*5, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "      optimizer=Adam(learning_rate),\n",
    "      metrics=['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Callback Function for Saving Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# # Include the epoch in the file name (uses `str.format`)\n",
    "# checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# batch_size = 1024\n",
    "# epochs=25\n",
    "\n",
    "# # Create a callback that saves the model's weights every 5 epochs\n",
    "# cp_callback = ModelCheckpoint(filepath=checkpoint_path, \n",
    "#                               verbose=1, \n",
    "#                               save_weights_only=True, \n",
    "#                               save_freq=5*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 21, 300)           60000     \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 21, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 21, 1725)          884925    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 21, 345)           595470    \n",
      "=================================================================\n",
      "Total params: 4,256,043\n",
      "Trainable params: 4,256,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 99259 samples, validate on 11029 samples\n",
      "Epoch 1/25\n",
      "99259/99259 [==============================] - 82s 823us/step - loss: 2.6701 - acc: 0.4654 - val_loss: 1.7598 - val_acc: 0.5570\n",
      "Epoch 2/25\n",
      "99259/99259 [==============================] - 80s 807us/step - loss: 1.4117 - acc: 0.6169 - val_loss: 1.1142 - val_acc: 0.6909\n",
      "Epoch 3/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 1.0215 - acc: 0.7060 - val_loss: 0.8542 - val_acc: 0.7451\n",
      "Epoch 4/25\n",
      "99259/99259 [==============================] - 80s 808us/step - loss: 0.9582 - acc: 0.7214 - val_loss: 0.7135 - val_acc: 0.7809\n",
      "Epoch 5/25\n",
      "99259/99259 [==============================] - 80s 807us/step - loss: 0.9297 - acc: 0.7371 - val_loss: 0.9085 - val_acc: 0.7338\n",
      "Epoch 6/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.6102 - acc: 0.8152 - val_loss: 0.4668 - val_acc: 0.8570\n",
      "Epoch 7/25\n",
      "99259/99259 [==============================] - 80s 808us/step - loss: 0.5085 - acc: 0.8468 - val_loss: 0.4502 - val_acc: 0.8658\n",
      "Epoch 8/25\n",
      "99259/99259 [==============================] - 80s 807us/step - loss: 0.3137 - acc: 0.9045 - val_loss: 0.2643 - val_acc: 0.9185\n",
      "Epoch 9/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.2238 - acc: 0.9307 - val_loss: 0.2174 - val_acc: 0.9320\n",
      "Epoch 10/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.1732 - acc: 0.9469 - val_loss: 0.1562 - val_acc: 0.9536\n",
      "Epoch 11/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.1398 - acc: 0.9576 - val_loss: 0.1445 - val_acc: 0.9572\n",
      "Epoch 12/25\n",
      "99259/99259 [==============================] - 80s 811us/step - loss: 0.1196 - acc: 0.9635 - val_loss: 0.1210 - val_acc: 0.9640\n",
      "Epoch 13/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.1000 - acc: 0.9692 - val_loss: 0.1071 - val_acc: 0.9684\n",
      "Epoch 14/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.0894 - acc: 0.9723 - val_loss: 0.0982 - val_acc: 0.9708\n",
      "Epoch 15/25\n",
      "99259/99259 [==============================] - 80s 811us/step - loss: 0.0847 - acc: 0.9737 - val_loss: 0.0927 - val_acc: 0.9721\n",
      "Epoch 16/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.2811 - acc: 0.9279 - val_loss: 1.2349 - val_acc: 0.6546\n",
      "Epoch 17/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.4009 - acc: 0.8784 - val_loss: 0.1505 - val_acc: 0.9569\n",
      "Epoch 18/25\n",
      "99259/99259 [==============================] - 81s 811us/step - loss: 0.1094 - acc: 0.9673 - val_loss: 0.1039 - val_acc: 0.9690\n",
      "Epoch 19/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.0807 - acc: 0.9756 - val_loss: 0.0872 - val_acc: 0.9743\n",
      "Epoch 20/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.0712 - acc: 0.9784 - val_loss: 0.0860 - val_acc: 0.9752\n",
      "Epoch 21/25\n",
      "99259/99259 [==============================] - 81s 812us/step - loss: 0.0615 - acc: 0.9813 - val_loss: 0.0777 - val_acc: 0.9772\n",
      "Epoch 22/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.3404 - acc: 0.9039 - val_loss: 0.1528 - val_acc: 0.9580\n",
      "Epoch 23/25\n",
      "99259/99259 [==============================] - 80s 809us/step - loss: 0.0893 - acc: 0.9744 - val_loss: 0.0862 - val_acc: 0.9756\n",
      "Epoch 24/25\n",
      "99259/99259 [==============================] - 81s 812us/step - loss: 0.0600 - acc: 0.9821 - val_loss: 0.0738 - val_acc: 0.9793\n",
      "Epoch 25/25\n",
      "99259/99259 [==============================] - 80s 810us/step - loss: 0.0501 - acc: 0.9849 - val_loss: 0.0704 - val_acc: 0.9801\n"
     ]
    }
   ],
   "source": [
    "# Train the final model with 10% of validation data.\n",
    "\n",
    "# Pad English sentences to Maximum French sentence lenth which is longer. This is for the consistency of the input / output tensor sizes. \n",
    "tmp_x = pad(train_english, max_french_sequence_length)\n",
    "\n",
    "# For embedding, 1 is added to each vocability to represent <PAD>.\n",
    "final = model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size+1,\n",
    "    french_vocab_size+1)\n",
    "\n",
    "# # Save the weights using the `checkpoint_path` format\n",
    "# final.save_weights(checkpoint_path.format(epoch=0))\n",
    "# final.fit(tmp_x, train_french, batch_size=1024, callbacks=[cp_callback], epochs=25, validation_split=0.1)\n",
    "\n",
    "final.fit(tmp_x, train_french, batch_size=1024, epochs=25, validation_split=0.1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model achieved over 98% accuracy for both tratining and validation data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "final.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Output of the Trained Model.\n",
    "The model training is done. Its output is logits which need to be converted to French words, then join the words back to sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that convert the output word ids into French sentences. \n",
    "# Because the neural network will generate the logits (word ids) as the output.\n",
    " \n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# The model's translation of the first sample of the dataset. \n",
    "\n",
    "text = logits_to_text(final.predict(tmp_x[:1])[0], french_tokenizer)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared to the target (label) sentence of the sample.\n",
    "\n",
    "french_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Using Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = final\n",
    "    sentences = pad(x, max_french_sequence_length)\n",
    "    \n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "    \n",
    "\n",
    "    return predictions\n",
    "\n",
    "predictions = final_predictions(test_english, test_french, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Predictions to Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "californie est agréable pendant l' hiver mais il est généralement chaud au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Label: \n",
      "californie est agréable pendant l' hiver mais il est généralement chaud au printemps <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# The first sample of the test dataset.\n",
    "\n",
    "# Predictsion from the model are logits. Convert them to words.\n",
    "pred = logits_to_text(predictions[0], french_tokenizer)\n",
    "print('Prediction: \\n{}'.format(pred))\n",
    "\n",
    "# Test Dataset of French sentences are in word ID format. Convert them back to words.\n",
    "id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "id_to_word[0] = '<PAD>'\n",
    "\n",
    "label = ' '.join([id_to_word[np.max(x)] for x in test_french[0]])\n",
    "print('Label: \\n{}'.format(label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
